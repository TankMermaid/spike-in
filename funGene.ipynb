{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "funGene.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bCUQsVXrQVrqJCvYOIU3Wj0fPh3nexOw",
      "authorship_tag": "ABX9TyODHcOkr8+nmAPDDBwFmPHZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TankMermaid/spike-in/blob/master/funGene.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSzIjVoM-8A8"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "\n",
        "__Author__= 'Tank'\n",
        "__Email__ = 'xnzhang@genetics.ac.cn'\n",
        "__Version__=1.0\n",
        "\n",
        "\n",
        "\"\"\"This script aims to scrape some website table-format \\\n",
        "data and apply in analysing some basic statistics and visualization\"\"\"\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "url=\"http://fungene.cme.msu.edu/\"\n",
        "# new_dic = {}\n",
        "\n",
        "\n",
        "def get_span_name(table_body):\n",
        "  span_names=[]\n",
        "  for tb in table_body[1:]:\n",
        "    # print(type(tb))\n",
        "    rows = tb.find_all('td',{\"style\": True} )\n",
        "    print(\"here is a tr block td data\")\n",
        "    # print(rows)\n",
        "    print(len(rows))\n",
        "    for r in rows:\n",
        "      span=r.find_all('span',{\"class\":\"hilite\"})\n",
        "      span_name=\"\".join([y.text.strip() for y in span])\n",
        "      # print(span_name)\n",
        "      # print(type(span_name))\n",
        "      if len(span_name) >= 1:\n",
        "        span_names.append(span_name)\n",
        "        # print(span_names)\n",
        "  return span_names\n",
        "\n",
        "def get_table_number(table_body):\n",
        "  for tb in table_body:\n",
        "    print(\"What you are scrapying is a table...\")\n",
        "    # print(tb)\n",
        "    print(\"all the table number is %d\"%(len(table_body)))\n",
        "\n",
        "def get_gene_url_list(tds):\n",
        "  ## parallel in multiple colab running \n",
        "  for td in tds:\n",
        "    # span=td.find_all('span',{\"class\":\"gene\"})\n",
        "    span=td.find_all('span')\n",
        "    # span_name=[y.text.strip() for y in span]\n",
        "    # print(span)# \n",
        "    td_name=td.find('span',{\"class\":\"hilite\"}).text\n",
        "    # print(td_name)\n",
        "    cwd=os.getcwd()\n",
        "    try:\n",
        "      os.makedirs(os.path.join(cwd,\"gdrive/My Drive/\",td_name),exist_ok=True) ## mkdir and mkdirs and makedirs ## join and os.path.join\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    link_list=[]\n",
        "    gene_list=[]\n",
        "    for href in span:\n",
        "      link = href.find_all('a')\n",
        "      links=\"\".join([url + x.get('href') for x in link])\n",
        "      # print(links)\n",
        "      if len(links) >0:\n",
        "        link_list.append(links)\n",
        "\n",
        "      genes=\"\".join([y.text.strip() for y in link])\n",
        "      if len(genes) >0:\n",
        "        gene_list.append(genes)\n",
        "    # print(link_list)\n",
        "    # print(gene_list)\n",
        "    # print(len(gene_list))\n",
        "    # print(len(link_list))\n",
        "  return link_list, gene_list\n",
        "\n",
        "def get_page_number(gene_list,link_list):\n",
        "  gene_url_map = dict(zip(gene_list,link_list))  ## test\n",
        "  print(gene_url_map)\n",
        "  for k,v in gene_url_map.items():\n",
        "    print(k)\n",
        "    print(v)\n",
        "    response_sub= requests.get(v)\n",
        "    html_sub=response_sub.content.decode(\"gbk\")\n",
        "    bs_sub = BeautifulSoup(html_sub, \"html.parser\")\n",
        "    # rows_sub = [tag for tag in bs_sub.find_all('td') if tag.text == \"Page\"]\n",
        "    rows_sub = bs_sub.find_all('td')\n",
        "    # td_page= re.findall(r'<td>Page:.*?</td>',rows_sub)\n",
        "    for i in rows_sub:\n",
        "      # a=[x.find('a') for x in i if x.find('a') is not None]\n",
        "      page_a_hit =i.find_all(\"a\")\n",
        "      \n",
        "      page_href=[x.get('href')  for x in page_a_hit if x.get('href').startswith(\"?hmm_id=\")]\n",
        "      if len(page_href) >0:\n",
        "        print(page_href)\n",
        "        page_length=len(p_href)+1\n",
        "        print(\"scrapying page number is %d\" %(page_length))\n",
        "    # print(\"here is a page\")\n",
        "    # print(td_page)\n",
        "\n",
        "\n",
        "def get_data(url):\n",
        "  response= requests.get(url)\n",
        "  html=response.content.decode(\"gbk\")\n",
        "    \n",
        "  bs=BeautifulSoup(html, \"html.parser\")\n",
        "  table_body=bs.find(\"table\")\n",
        "  # print(table_body)\n",
        "  # table_body=bs.find_all(\"table\")\n",
        "  ## get table number\n",
        "  get_table_number(table_body)\n",
        "\n",
        "  # get span name\n",
        "  # span_names=get_span_name(table_body)\n",
        "\n",
        "   ## get td list\n",
        "  # td_list=[]\n",
        "  # for tb in table_body[1::]:\n",
        "  tb = table_body\n",
        "    # print(type(tb))\n",
        "  tds = tb.find_all('td',{\"style\": True} )\n",
        "  print(\"here is a tr block td data\")\n",
        "  # print(rows)\n",
        "  print(\"all the table section number is %d\" %(len(tds)))\n",
        "\n",
        "  # link_list,gene_list = get_gene_url_list(tds)\n",
        "  for td in tds[0:2]:\n",
        "    new_dic={}\n",
        "    # span=td.find_all('span',{\"class\":\"gene\"})\n",
        "    span=td.find_all('span')\n",
        "    # span_name=[y.text.strip() for y in span]\n",
        "    # print(span)# \n",
        "    td_name=td.find('span',{\"class\":\"hilite\"}).text\n",
        "    # print(td_name)\n",
        "    cwd=os.getcwd()\n",
        "    try:\n",
        "      os.makedirs(os.path.join(cwd,\"gdrive/My Drive/\",td_name),exist_ok=True) ## mkdir and mkdirs and makedirs ## join and os.path.join\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    link_list=[]\n",
        "    gene_list=[]\n",
        "    for href in span:\n",
        "      link = href.find_all('a')\n",
        "      links=\"\".join([url + x.get('href') for x in link])\n",
        "      # print(links)\n",
        "      if len(links) >0:\n",
        "        link_list.append(links)\n",
        "\n",
        "      genes=\"\".join([y.text.strip() for y in link])\n",
        "      if len(genes) >0:\n",
        "        gene_list.append(genes) \n",
        "    \n",
        "    \n",
        "    # get_next_layer(gene_list,link_list)\n",
        "    gene_url_map = dict(zip(gene_list,link_list))  ## test\n",
        "    print(gene_url_map)\n",
        "    for k,v in gene_url_map.items():\n",
        "      print(k)\n",
        "      print(v)\n",
        "      response_sub= requests.get(v)\n",
        "      html_sub=response_sub.content.decode(\"gbk\")\n",
        "      bs_sub = BeautifulSoup(html_sub, \"html.parser\")\n",
        "      # rows_sub = [tag for tag in bs_sub.find_all('td') if tag.text == \"Page\"]\n",
        "      rows_sub = bs_sub.find_all('td')\n",
        "      # td_page= re.findall(r'<td>Page:.*?</td>',rows_sub)\n",
        "      pageN=0 ##init\n",
        "      for i in rows_sub:\n",
        "        # a=[x.find('a') for x in i if x.find('a') is not None]\n",
        "        page_a_hit =i.find_all(\"a\")\n",
        "        \n",
        "        page_href=[x.get('href')  for x in page_a_hit if x.get('href').startswith(\"?hmm_id=\")]\n",
        "        if len(page_href) >0:\n",
        "          print(page_href)\n",
        "          page_length=len(page_href)+1\n",
        "          print(\"scrapying page number is %d\" %(page_length))\n",
        "          pageN=page_length\n",
        "      \n",
        "      \n",
        "\n",
        "      page_gene_link=[] \n",
        "      for pg in range(1,pageN): \n",
        "        for i in rows_sub:\n",
        "          temp_dict = {}\n",
        "          page_a_hit =i.find_all(\"a\")\n",
        "          gene_prot_href=\"\".join([url + x.get('href')+ \"&page=\"+ str(pg) for x in page_a_hit if x.get('href').startswith(\"gpprotdata\")])\n",
        "          gene_prot_name=\"\".join([y.text.strip() for y in page_a_hit])\n",
        "\n",
        "          # print(gene_prot_href)\n",
        "          # print(gene_prot_name)\n",
        "          if len(gene_prot_href) >0 and len(gene_prot_name)>0:\n",
        "            print(gene_prot_href)\n",
        "            page_gene_link.append(gene_prot_href)\n",
        "            \n",
        "            response_sub_sub= requests.get(gene_prot_href)\n",
        "            html_sub_sub=response_sub_sub.content.decode(\"gbk\")\n",
        "            bs_sub_sub = BeautifulSoup(html_sub_sub, \"html.parser\")\n",
        "            # print(bs_sub_sub)\n",
        "            rows_sub_sub = bs_sub_sub.find_all('a')\n",
        "            rows_sub_sub=rows_sub_sub[1:]\n",
        "            # print(rows_sub_sub)\n",
        "            ncbi_href=[x.get('href')  for x in rows_sub_sub if x.get('href').startswith(\"http://www.ncbi\")]\n",
        "            # ncbi_name=[y.text.strip() for y in rows_sub_sub]\n",
        "            print(ncbi_href[0])\n",
        "            # print(ncbi_name)\n",
        "            k_s = td_name + \"-\" + k  + \"-\" + gene_prot_name + \"-\" + str(pg)\n",
        "            print(k_s)\n",
        "            print(ncbi_href[0])\n",
        "            temp_dict[k_s] = ncbi_href[0]\n",
        "            new_dic[k_s]=ncbi_href[0] ## 0 fasta 1 genebank\n",
        "            cwd=os.getcwd()\n",
        "            print(td_name)\n",
        "            print(cwd)\n",
        "            try:\n",
        "              dir_path = os.path.join(cwd,\"gdrive/My Drive\",td_name,k,gene_prot_name,str(pg))\n",
        "              os.makedirs(dir_path,exist_ok=True) ## mkdir and mkdirs and makedirs ## join and os.path.join\n",
        "              jsObj = json.dumps(temp_dict, indent=4)\n",
        "              fh = open(os.path.join(dir_path,'fungene_url.json'), 'w') \n",
        "              fh.write(jsObj) \n",
        "              fh.close()\n",
        "            except:\n",
        "              pass\n",
        "            # print(gene_prot_name)\n",
        "        # if len(page_gene_link) >0:\n",
        "          # print(page_gene_link)\n",
        "\n",
        "    try:\n",
        "      dir_path = os.path.join(cwd,\"gdrive/My Drive\",td_name,k,\"summry\")\n",
        "      os.makedirs(dir_path,exist_ok=True) ## mkdir and mkdirs and makedirs ## join and os.path.join\n",
        "      jsObj = json.dumps(new_dict, indent=4)\n",
        "      fh = open((k+'fungene_url.json'), 'w') \n",
        "      fh.write(jsObj) \n",
        "      fh.close()\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "        # for lk in page_gene_link:\n",
        "        #   response_sub_sub= requests.get(lk)\n",
        "        #     html_sub_sub=response_sub_sub.content.decode(\"gbk\")\n",
        "        #     bs_sub_sub = BeautifulSoup(html_sub_sub, \"html.parser\")\n",
        "        #     # print(bs_sub_sub)\n",
        "        #     rows_sub_sub = bs_sub_sub.find_all('a')\n",
        "        #     rows_sub_sub=rows_sub_sub[1:]\n",
        "        #     # print(rows_sub_sub)\n",
        "        #     ncbi_href=[x.get('href')  for x in rows_sub_sub if x.get('href').startswith(\"http://www.ncbi\")]\n",
        "        #     ncbi_name=[y.text.strip() for y in rows_sub_sub]\n",
        "        #     # print(ncbi_href[1])\n",
        "        #     # print(ncbi_name)\n",
        "        #     k_s = k + \"-\" + k1\n",
        "        #     print(k_s)\n",
        "        #     print(ncbi_href[0])\n",
        "        #     new_dic[k_s]=ncbi_href[0] ## 0 fasta 1 genebank\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "          # for v1 in c_sub.items():\n",
        "          #       # print(v1)\n",
        "          #       response_sub_sub= requests.get(v1)\n",
        "          #       html_sub_sub=response_sub_sub.content.decode(\"gbk\")\n",
        "          #       bs_sub_sub = BeautifulSoup(html_sub_sub, \"html.parser\")\n",
        "          #       # print(bs_sub_sub)\n",
        "          #       rows_sub_sub = bs_sub_sub.find_all('a')\n",
        "          #       rows_sub_sub=rows_sub_sub[1:]\n",
        "          #       # print(rows_sub_sub)\n",
        "          #       ncbi_href=[x.get('href')  for x in rows_sub_sub if x.get('href').startswith(\"http://www.ncbi\")]\n",
        "          #       ncbi_name=[y.text.strip() for y in rows_sub_sub]\n",
        "          #       # print(ncbi_href[1])\n",
        "          #       # print(ncbi_name)\n",
        "          #       k_s = k + \"-\" + k1\n",
        "          #       print(k_s)\n",
        "          #       print(ncbi_href[0])\n",
        "          #       new_dic[k_s]=ncbi_href[0] ## 0 fasta 1 genebank\n",
        "                \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "      \n",
        "\n",
        "        \n",
        "    # print(rows_sub)\n",
        "\n",
        "        # p_names=[]\n",
        "        # p_hrefs=[]\n",
        "        # for line in rows_sub: ## test\n",
        "    # for line in rows_sub:\n",
        "          # print(\"hello\")\n",
        "  \n",
        "\n",
        "  # ## get td list\n",
        "  # td_list=[]\n",
        "  # # for tb in table_body[1::]:\n",
        "  # for tb in table_body:\n",
        "  #   # print(type(tb))\n",
        "  #   tds = tb.find_all('td',{\"style\": True} )\n",
        "  #   print(\"here is a tr block td data\")\n",
        "  #   # print(rows)\n",
        "  #   print(len(tds))\n",
        "   \n",
        "  #   for td in tds:\n",
        "  #     span=td.find_all('span',{\"class\":\"gene\"})\n",
        "  #     # span_name=[y.text.strip() for y in span]\n",
        "  #     print(span)# \n",
        "\n",
        "\n",
        "  #   # rows = table_body[1].find_all('td')\n",
        "  #   # print(rows)\n",
        "  #   # print(len(rows))\n",
        "\n",
        "  #   # names=[]\n",
        "  #   # hrefs=[]\n",
        "  #   # row and column handle\n",
        "  #   for row in rows[3:4]:  ## td3\n",
        "  #     cols=row.find_all('a')\n",
        "  #     href=[url + x.get('href') for x in cols if x.get('href') is not None ]\n",
        "  #     # conc_href=url+href\n",
        "  #     name=[y.text.strip() for y in cols]\n",
        "  #     # cols=[x.text.strip() for x in cols]\n",
        "  #     print(len(href))\n",
        "  #     print(len(name))\n",
        "  #     # names.append(name)\n",
        "  #     # hrefs.append(href)\n",
        "  #     # c=dict(zip(name,href))\n",
        "  #     c=dict(zip(name,href))  ## test\n",
        "  #     # print(hrefs)\n",
        "  #     # print(names)\n",
        "  #     # print(list(c))\n",
        "  #     # print(c)\n",
        "  #     for k,v in c.items():\n",
        "  #       print(k)\n",
        "  #       print(v)\n",
        "  #       response_sub= requests.get(v)\n",
        "  #       html_sub=response_sub.content.decode(\"gbk\")\n",
        "  #       bs_sub = BeautifulSoup(html_sub, \"html.parser\")\n",
        "  #       # print(bs_sub)\n",
        "  #       # table_body_sub = bs_sub.find('tr')\n",
        "  #       rows_sub = bs_sub.find_all('td')\n",
        "  #       # print(rows_sub)\n",
        "\n",
        "  #       # p_names=[]\n",
        "  #       # p_hrefs=[]\n",
        "  #       # for line in rows_sub: ## test\n",
        "  #       for line in rows_sub:\n",
        "  #         # print(\"hello\")\n",
        "  #         # print(len(rows_sub))\n",
        "  #         # print(\"hi\")\n",
        "  #         cols_sub=line.find_all('a')\n",
        "  #         # print(\"hello\")\n",
        "  #         # print(cols_sub)\n",
        "  #         # print(\"hi\")\n",
        "  #         if len(cols_sub) == 1:\n",
        "  #           p_href=[url + x.get('href')  for x in cols_sub if x.get('href').startswith(\"gpprotdata\")]\n",
        "  #           p_name=[y.text.strip() for y in cols_sub]\n",
        "  #           # print(\"hello\")\n",
        "  #           # print(p_href)\n",
        "  #           # print(p_name)\n",
        "  #           if len(p_href) != 0:\n",
        "  #             c_sub=dict(zip(p_name,p_href))\n",
        "  #             # print(c_sub)\n",
        "              \n",
        "  #             for k1,v1 in c_sub.items():\n",
        "  #               # print(v1)\n",
        "  #               response_sub_sub= requests.get(v1)\n",
        "  #               html_sub_sub=response_sub_sub.content.decode(\"gbk\")\n",
        "  #               bs_sub_sub = BeautifulSoup(html_sub_sub, \"html.parser\")\n",
        "  #               # print(bs_sub_sub)\n",
        "  #               rows_sub_sub = bs_sub_sub.find_all('a')\n",
        "  #               rows_sub_sub=rows_sub_sub[1:]\n",
        "  #               # print(rows_sub_sub)\n",
        "  #               ncbi_href=[x.get('href')  for x in rows_sub_sub if x.get('href').startswith(\"http://www.ncbi\")]\n",
        "  #               ncbi_name=[y.text.strip() for y in rows_sub_sub]\n",
        "  #               # print(ncbi_href[1])\n",
        "  #               # print(ncbi_name)\n",
        "  #               k_s = k + \"-\" + k1\n",
        "  #               print(k_s)\n",
        "  #               print(ncbi_href[0])\n",
        "  #               new_dic[k_s]=ncbi_href[0] ## 0 fasta 1 genebank\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "  #         else:\n",
        "  #           pass\n",
        "\n",
        "  #   return new_dic\n",
        "\n",
        "def main():\n",
        "  get_data(url)  \n",
        "\n",
        "  # jsObj = json.dumps(dictObj, indent=4)\n",
        "  # fileObject = open('./fungene.json', 'w') \n",
        "  # fileObject.write(jsObj) \n",
        "  # fileObject.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    # print(__name__)\n",
        "     \n",
        "  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQCItQ8XPSEC"
      },
      "source": [
        "os.makedirs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V99HrZ8WJG3Q",
        "outputId": "5aa50974-f92e-413c-d8be-d68d26e8a6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "import os\n",
        "# print(os.path)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuY9yeJ5R9uA"
      },
      "source": [
        "!ls -l drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D41216HMlwNk"
      },
      "source": [
        "# js=pd.read_json(\"./fungene.json\",encoding='utf-8',orient= \"index\")\n",
        "\n",
        "# type(js)\n",
        "\n",
        "with open(\"./fungene.json\") as f:\n",
        "  js = json.load(f)\n",
        "  print(js)\n",
        "  # t=type(js)\n",
        "  # print(t)\n",
        "  for k, v in js.items():\n",
        "    cmd = [\"wget\",v,\"-O\", '{:s}/{:s}.fasta'.format(k,k)]\n",
        "    print(cmd)\n",
        "    subprocess.run(cmd)\n",
        "  f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSydrw0-3HyS"
      },
      "source": [
        "!python drive/My\\ Drive/funGeneDld.py 2>&1 >> drive/My\\ Drive/dld1.log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}